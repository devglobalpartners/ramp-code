{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check: that there are no duplicate images in your dataset.\n",
    "\n",
    "To do a quick automated check for duplicate images, we'll work from this article: [Detect and remove duplicate images from a dataset for deep learning.](https://www.pyimagesearch.com/2020/04/20/detect-and-remove-duplicate-images-from-a-dataset-for-deep-learning/). \n",
    "\n",
    "The article explains:\n",
    "- why it is important to check for duplicates\n",
    "- how to check for duplicates in a large dataset using image hashing\n",
    "\n",
    "An image hash is a function that maps an image to an value that is identical for two identical images. The space of possible values of the hash function, however, should be so large that it is unlikely that two different images would map to the same value.    \n",
    "\n",
    "This article also gives an implementation of a very simple image hashing function, _dhash_(). The space of possible output values of _dhash_() is a very large but finite subset of the positive integers. In addition, the function is designed to be somewhat invariant to image rescaling: this means that if a dataset contains two identical RGB images, but one has had all of its values in all bands multiplied by a single positive value, then their hashes will be identical. \n",
    "\n",
    "This hash function is not designed to be robust to a bad actor who is actually trying to mess up your dataset (by, for example, adding the same image twice, with two different labels). It's intended to find duplicate images that landed in the dataset by accident. This is a common occurrence.\n",
    "\n",
    "The larger hashSize is, the larger the space of possible hashes, and the lower is the likelihood of 'hash-collisions' - images that are different but have the same hash value. \n",
    "\n",
    "But keep in mind that the space of possible hash values for the default hashSize, 8, is huge (2^64). This is big enough for most  purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "import numpy as np\n",
    "RAMP_HOME = os.environ[\"RAMP_HOME\"]\n",
    "\n",
    "from osgeo import gdal, gdalnumeric as gdn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added this code to prevent mysterious out of memory problems with the GPU.\n",
    "# see this webpage for explanation: \n",
    "# https://stackoverflow.com/questions/43147983/could-not-create-cudnn-handle-cudnn-status-internal-error\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # tensorflow to only print errors please\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ramp.utils.img_utils import gdal_get_mask_tensor, gdal_get_image_tensor, convert_RGB_array_to_grayscale, dhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pathnames for all images in the dataset. \n",
    "\n",
    "dataset = Path(RAMP_HOME) / 'ramp-code/notebooks/sample-data/training_data/chips'\n",
    "img_files = [str(fn) for fn in sorted(dataset.glob('**/*.tif'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The hashing method first converts the image to a grayscale image. \n",
    "\n",
    "First we look at the color image, then its grayscale version, for a sanity check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first look at color image\n",
    "f1 = gdal_get_image_tensor(img_files[6])\n",
    "plt.imshow(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view after conversion to grayscale\n",
    "gray1 = convert_RGB_array_to_grayscale(f1)\n",
    "plt.imshow(gray1, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates using the hash function. \n",
    "\n",
    "This is done the same way as described in the reference article. \n",
    "\n",
    "Create a dictionary with the hash value of an image as the key, and lists of image paths as the value. \n",
    "Any hash values with more than one image path in the list denotes a duplicate image.\n",
    "\n",
    "No duplicates should be found. If any are found, they should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dictionary with dhash-values as keys, and a list of files with that dhash-value as values\n",
    "hashes = {}\n",
    "for filepath in img_files:\n",
    "    c_img = gdal_get_image_tensor(filepath)\n",
    "    h = dhash(c_img)\n",
    "    h_imglist = hashes.get(h, [])\n",
    "    h_imglist.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, check the data set for duplicates\n",
    "found_dups = False\n",
    "for hashval, pathlist in hashes.items():\n",
    "    if len(pathlist) > 1:\n",
    "\n",
    "        # found a hash matching more than one image\n",
    "        found_dups = True\n",
    "        print(f\"found duplicate images:\")\n",
    "        for p in pathlist: \n",
    "            print(p)\n",
    "        print(\"\")\n",
    "        \n",
    "if not found_dups:\n",
    "    print(\"No duplicates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Created for ramp project, August 2022\n",
    "##### Author: carolyn.johnston@dev.global"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
