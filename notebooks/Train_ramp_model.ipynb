{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ramp project: training notebook\n",
    "\n",
    "This notebook is intended to walk you through the ramp training process, explain what's going on at every stage, and help you understand what's in a training configuration file. \n",
    "\n",
    "We run a (very) toy training example that uses a saved model, 32 training samples, and 16 validation samples.\n",
    "\n",
    "To use this notebook for training, change the training configuration file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the RAMP_HOME environment variable for your environment.\n",
    "\n",
    "We need to first make sure that the RAMP_HOME environment variable is properly defined, for whatever environment we are running in.\n",
    "\n",
    "RAMP_HOME is the parent directory where you have placed the ramp-code and ramp-data directories, containing (respectively) all of the ramp codebase, and all of the data accessed by your ramp session.\n",
    "\n",
    "If you are running in the ramp docker container, the RAMP_HOME variable has already been set in the container.\n",
    "\n",
    "If you are running on a local server, you will set it to the directory above your ramp-code and ramp-data directories. \n",
    "\n",
    "If you are running on Colab, you will need to set the variable below (uncomment the line that begins with %env). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For running on COLAB\n",
    "%env RAMP_HOME=/content/drive/MyDrive/RAMP_HOME\n",
    "\n",
    "# For running in the ramp docker container\n",
    "# %env RAMP_HOME=/tf\n",
    "\n",
    "# For running on your own host server: something like\n",
    "#%env RAMP_HOME=/home/carolyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing python dependencies, including functions needed from the ramp codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Note: this suppresses warning and other less urgent messages,\n",
    "# and only allows errors to be printed.\n",
    "# Comment this out if you are having mysterious problems, so you can see all messages.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# this variable must be defined. It is the parent of the 'ramp-code' directory.\n",
    "RAMP_HOME = os.environ[\"RAMP_HOME\"]\n",
    "\n",
    "# import ramp dependencies.\n",
    "from ramp.training.augmentation_constructors import get_augmentation_fn \n",
    "from ramp.training import callback_constructors\n",
    "from ramp.training import model_constructors\n",
    "from ramp.training import optimizer_constructors\n",
    "from ramp.training import metric_constructors\n",
    "from ramp.training import loss_constructors\n",
    "\n",
    "from ramp.data_mgmt.data_generator import training_batches_from_gtiff_dirs, test_batches_from_gtiff_dirs\n",
    "from ramp.utils.misc_ramp_utils import log_experiment_to_file, get_num_files\n",
    "from ramp.models.effunet_1 import get_effunet\n",
    "from ramp.utils.model_utils import get_best_model_value_and_epoch\n",
    "import ramp.utils.log_fields as lf\n",
    "\n",
    "import segmentation_models as sm\n",
    "sm.set_framework('tf.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on callback functions\n",
    "\n",
    "Segmentation model training makes use of multiple callback functions, which create and track events at regular points during the training process.\n",
    "\n",
    "Some callbacks should be used in every training run. \n",
    "\n",
    "- Tensorboard callback (get_tb_callback_fn): Used for tracking model training metrics using Tensorboard (i.e., loss and accuracy). Always essential for training. \n",
    "- Prediction logging (get_pred_logging_callback_fn): Used for displaying changing building prediction results, using Tensorboard. Essential. \n",
    "- Model checkpoint logging callback (get_model_checkpt_callback_fn): Used for saving copies of the best-performing models during training. Essential, since saving good models is the point of the exercise. \n",
    "\n",
    "Some callbacks are used frequently for training.\n",
    "\n",
    "- Early stopping callback (get_early_stopping_callback_fn): Stops the training process before all epochs are completed if the training metrics stop improving. Very useful. \n",
    "\n",
    "In addition, other callback-based functionality was tested, and those were left in the codebase for others to try.\n",
    "\n",
    "- get_clr_callback_fn: Used for cyclic learning schedules, which change the learning rate as training proceeds. Sometimes provides an accuracy boost if training is prone to getting stuck in non-optimal solutions.\n",
    "\n",
    "The list below shows all the callbacks currently available for use in ramp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([f for f in dir(callback_constructors) if f.startswith(\"get\") and f.endswith(\"callback_fn\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training configuration file\n",
    "\n",
    "Training runs are defined using json configuration files, which specify training datasets, training options, and hyperparameter choices. Sample config files are in the 'experiments' subdirectory of the codebase, organized in folders according to the training datasets specified.\n",
    "\n",
    "At the top level, the config file contains the following blocks:\n",
    "- *experiment_name*: simple, descriptive name for your own use\n",
    "- *discard_experiment*: set to 'true' if this is a test run that you don't want to keep any record of\n",
    "- *logging*: whether to log the experiment to a csv file, and what fields to log\n",
    "- *datasets*: directories where training and validation datasets are stored\n",
    "- *num_classes*: 2 for binary masks, 4 for multichannel masks\n",
    "- *num_epochs*: number of cycles for the training run. Overridden if using early stopping. \n",
    "- *batch_size*: number of samples per 'batch'. Smaller batches are needed for smaller gpus. Larger batches stabilize the training metrics, and result in fewer iterations and shorter training times.\n",
    "- *input_img_shape*: (H,W) of the input images.\n",
    "- *output_img_shape*: (H,W) of output masks. \n",
    "- *loss*: the loss function to use in training, and any parameters to use in its construction.\n",
    "- *metrics*: the accuracy functions to track during training, and parameters to use in their construction.\n",
    "- *optimizer*: choice of optimizer to use in training, and optimizer parameters. \n",
    "- *model*: choice of model to use in training (currently just EfficientUnet), and parameters to use in its construction, including the variety of EfficientNet to use for the encoder. \n",
    "- *saved_model*: whether to resume training from a saved model, and if so, the location of the saved model.  \n",
    "- *augmentation*: whether to use image augmentation (e.g., random rotations, random changes in color) to regularize training. If so, which augmentations to apply, and parameters needed to construct them. \n",
    "- *early_stopping*: whether to use early stopping, and parameters needed to define early stopping rules. \n",
    "- *cyclic_learning_scheduler*: whether to use a cyclic learning scheduler (generally no unless you're a power user), and parameters needed to construct one.\n",
    "- *tensorboard*: whether to log training metrics in tensorboard (always yes), with what frequency, and where to store logs.\n",
    "- *prediction_logging*: whether to log prediction samples in tensorboard (yes).\n",
    "- *model_checkpts*: whether to save 'best models' during training (always yes), and to where.\n",
    "- *random_seed*: an integer to use for the random seed, for reproducibility of results. \n",
    "\n",
    "## The contents of a sample configuration file are shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the configuration file\n",
    "\n",
    "This code block reads in a sample file. \n",
    "\n",
    "Change this to your own configuration file when you're doing a training run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = Path(RAMP_HOME)/\"ramp-code/data/sample_config.json\"\n",
    "\n",
    "with open(config_file) as jf:\n",
    "        cfg = json.load(jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment_name\n",
    "\n",
    "A simple, descriptive name for your own use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"experiment_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard_experiment\n",
    "\n",
    "Set to 'true' if this is a test run that you don't need or want to keep any record of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"discard_experiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging\n",
    "\n",
    "whether to log the experiment to a csv file. If so, what file to log to, and what information to log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "cfg[\"logging\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### datasets\n",
    "\n",
    "Paths to directories containing the training images, training masks, validation images, and validation masks. \n",
    "\n",
    "**Important note**\n",
    "\n",
    "All directory and file paths defined in a ramp config file are assumed to be *relative* paths, defined relative to the RAMP_HOME environment variable, which must be defined in any environment used to run ramp code. \n",
    "\n",
    "In the ramp docker image, RAMP_HOME is defined to be '/tf' (since the ramp docker image is based on a tensorflow docker image).\n",
    "\n",
    "Therefore, the training image directory at runtime will actually be: *RAMP_HOME/train_img_dir*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"datasets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### num_classes\n",
    "\n",
    "This training run is for a multichannel mask model with 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"num_classes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### num_epochs\n",
    "\n",
    "The number of times the training process will pass through the entire training dataset. \n",
    "\n",
    "I usually set this value very large, and employ early stopping during training. As noted below, early stopping will stop training if the metrics show that the model is no longer learning, even if the full number of epochs has not been reached.\n",
    "\n",
    "This example uses a little bit of toy data, so we only run two epochs, just to make sure the machinery works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch_size\n",
    "\n",
    "The number of samples to use per 'batch' (iteration of training). Smaller batches (say, size 4) are needed for smaller/older gpus. Larger batches stabilize the training metrics (i.e., they 'jump around' less), and result in fewer training iterations and shorter training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input_img_shape\n",
    "\n",
    "(H,W) of the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"input_img_shape\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output_img_shape\n",
    "\n",
    "(H,W) of the output masks. Generally but not always the same as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"output_img_shape\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss\n",
    "\n",
    "Defines the loss function, and parameters needed to use in its construction.\n",
    "\n",
    "Training a segmentation neural network means finding a solution that minimizes the difference between a truth and a predicted segmentation mask, for all the samples in the training data set. \"Differences\" between masks are measured, during training, by *loss functions*. \n",
    "\n",
    "Tweaking the definition of the loss function used during training can have a huge effect on the results. For example, if the loss function is defined so that any mistakes made on pixels at the boundaries of buildings increase the loss function more than mistakes made on the interior pixels of buildings, the trained model will try harder to get correct results on the boundaries.  \n",
    "\n",
    "Multiple loss functions were tested during ramp development, and the options were left in the code for others to try. \n",
    "\n",
    "**get_loss_fn_name**: the name of the function in the *loss_constructors* module that will construct and return the loss function (using any parameters you define). \n",
    "\n",
    "In this case, *get_sparse_categorical_crossentropy_fn()* will construct the sparse_categorical_crossentropy loss function and return it. \n",
    "\n",
    "**loss_fn_parms**: Since there are no additional parameters needed for constructing *sparse_categorical_crossentropy()*, this field is blank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list below shows all the loss functions available in the ramp code. Most of these were not extensively tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([f for f in dir(loss_constructors) if f.startswith(\"get\")and f.endswith(\"fn\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics\n",
    "\n",
    "Defines accuracy metrics to track during training, and parameters needed for their construction. \n",
    "\n",
    "Accuracy tracking allows us to monitor the accuracy of our model during training, on both the training and validation data. In general, the value of accuracy on the validation data will improve during the early phase of training, and then start to worsen when the model starts to overfit the training data. The value of accuracy on the training data will continue to improve the whole time a model is training.\n",
    "\n",
    "A user may track as many accuracy metrics as desired during training. \n",
    "\n",
    "The accuracy metric tracked during ramp training was sparse categorical accuracy, which reports the percentage of pixels in the truth mask that are correctly labeled by the model. It is very computationally efficient to compute, which makes it a good accuracy metric to use during training time. This accuracy metric is generally very high during training, around 97% on the validation data, but pixelwise metrics do not necessarily give a good feeling for the accuracy of building extractions. \n",
    "\n",
    "The metric we use to directly measure the accuracy of building extractions after training is the Spacenet building extraction metric, [F1 based on IoU@0.5](https://medium.com/the-downlinq/the-spacenet-metric-612183cc2ddb). This metric is too expensive to compute during training runs.\n",
    "\n",
    "**use_metrics**: should always be true.\n",
    "\n",
    "**get_metrics_fn_names**: this is a list of functions from the *metric_constructors* module that are used to construct the accuracy functions (it is a list because more than one metric can be tracked per training run).\n",
    "\n",
    "**metrics_fn_parms**: a list of parameter sets, of the same length as the get_metrics_fn_names list, to pass to the metric constructors. In this case, there is only one parameter set, and it is empty because sparse_categorical_accuracy does not need any parameters for construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output below shows all ramp's current options for training-time accuracy metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([f for f in dir(metric_constructors) if f.startswith(\"get\") and f.endswith(\"fn\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer\n",
    "\n",
    "Defines the optimization function to use in training, and any parameters needed for its construction.\n",
    "\n",
    "**get_optimizer_fn_name**: the name of the function in the optimizer_constructors module that will construct the optimizer for the training run. In this case, *get_adam_optimizer* will construct an Adam optimizer.\n",
    "\n",
    "**optimizer_fn_parms**: any parameters needed by the construction function. In this case, we want to set the learning rate for the Adam optimizer to 3E=04. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"optimizer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model\n",
    "\n",
    "Defines the choice of model to use in training, and parameters to use in its construction.\n",
    "\n",
    "The segmentation model used by ramp is a Unet with one of several EfficientNet encoder options, which may be specified in the *model_fn_parms* list.\n",
    "\n",
    "**get_model_fn_name**: the name of the function in the model_constructors module that will construct the model for the training run. Only 'get_effunet_model' is currently defined in ramp, which returns a Unet segmentation model with a configured EfficientNet encoder.\n",
    "\n",
    "**model_fn_parms**: parameters to pass to 'get_effunet_model'. Currently there are two: 'backbone', which defines the EfficientNet type to use for the Unet encoder, and 'classes', which is a list of the class names used for the segmentation problem.\n",
    "\n",
    "'backbone' is set to 'efficientnetb0', which is the smallest of the EfficientNet options. Options available are 'efficientnetb0' through 'efficientnetb7'; [documentation is here](https://github.com/qubvel/efficientnet#about-efficientnet-models).\n",
    "\n",
    "'classes' is a list containing 2 classes, for the binary segmentation problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list below shows all current options for segmentation models available in ramp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([f for f in dir(model_constructors) if f.startswith(\"get\")and f.endswith(\"model\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saved_model\n",
    "\n",
    "Ramp training includes an option to resume training from an existing model. \n",
    "\n",
    "In this case, *use_saved_model* is false, so we do not use a saved model. If this field were true, the training code would load the model in the \"saved_model_path\" field, and ignore the information in the 'model' section of the configuration. \n",
    "\n",
    "If a saved model is used, and the \"save_optimizer_state\" field is false, then the model will be recompiled. This means that all information about the training state of the saved model will be lost. Otherwise, training will resume from the model state as it was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"saved_model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### augmentation\n",
    "\n",
    "Whether to use image augmentations during training, in order to regularize training. If so, then this block also specifies augmentation choices, and the parameters needed for their construction.\n",
    "\n",
    "**aug_list**: the list of names of augmentation functions to use during training; \n",
    "\n",
    "**aug_parms** a list, of the same length as 'aug_list', of *sets* of parameters to be passed to the constructors of each augmentation function.\n",
    "\n",
    "In the code below, there are two augmentation functions being constructed. One is a random rotation, and the other is a random (slight) change of colors. Each has its own set of parameters which are specified for its construction. \n",
    "\n",
    "[This link](https://github.com/albumentations-team/albumentations#list-of-augmentations) points to the complete list of available augmentations, and the parameters required to construct them. Note that many augmentations (such as, for example, InvertImg) are not appropriate for use with remote sensing imagery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"augmentation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### early stopping\n",
    "\n",
    "Whether to use 'early stopping' during training. Early stopping stops training when a watched accuracy metric fails to improve over a sufficiently long time. \n",
    "\n",
    "For example, if the time series of validation loss values has reached its minimum value, and has failed to reach a new minimum after some (user-specified) number of new epochs, early stopping will stop the training even if the full number of epochs has not been run.\n",
    "\n",
    "**early_stopping_parms**: parameters to pass to the early stopping construction function, including which metric to monitor for the early stopping decision (*monitor*), and how many epochs to wait before stopping a training run that is not improving (*patience*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"early_stopping\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cyclic learning scheduler\n",
    "\n",
    "Whether to use a cyclic learning scheduler, which adjusts the value of the learning rate parameter during training in order to help the model escape local minima during training, and reach a more optimal final solution.\n",
    "\n",
    "In general, you won't need to use CLR schedulers during training unless you suspect that you are getting stuck with a non-optimal solution to your training problem.  \n",
    "\n",
    "[This article](https://pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/) provides a good introduction to cyclical learning rate schedulers. \n",
    "\n",
    "(Also note: If tuning a neural network's learning rate is a new idea, I recommend starting with [this article](https://pyimagesearch.com/2019/07/22/keras-learning-rate-schedules-and-decay/), which is the first in a 3-part series, of which the cyclic learning rate article linked above is the second.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"cyclic_learning_scheduler\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorboard\n",
    "\n",
    "Whether, and where, to log training metrics using the tensorboard utility. This option should always be turned on!\n",
    "\n",
    "The tensorboard utility allows you to monitor the status of training, and training loss and accuracy, during the training process. Since training takes a long time, having a window into the process is critical. \n",
    "\n",
    "**tb_logs_dir**: the parent directory of all tensorboard logging directories. Note that ramp code gives every training run a unique name using its time stamp, so every training run will log to a unique subdirectory of the 'tb_logs_dir' directory.\n",
    "\n",
    "**tb_callback_parms**: a list of parameters to be passed to the Tensorboard callback function. These mostly control the frequency of update of the Tensorboard output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"tensorboard\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction_logging\n",
    "\n",
    "Whether to add the display of predicted segmentation images to the Tensorboard display. This should always be true, since it lends enormous insight into the results of the training process. Prediction images should improve visibly during training, and changes should stabilize (i.e., not change wildly from epoch to epoch) as training continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"prediction_logging\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model_checkpts\n",
    "\n",
    "Whether and where to save 'best models' during training. This should always be true. \n",
    "\n",
    "Currently, the 'best_models' are defined as those that have the current maximum value of the validation accuracy metric. \n",
    "\n",
    "**model_checkpt_dir**: the parent directory of all model checkpoint logging directories. Note that ramp code gives every training run a unique name using its time stamp, so every training run will store its 'best models' to a unique subdirectory of the 'model_checkpt_dir' directory.\n",
    "\n",
    "**model_checkpt_callback_parms:** List of parameters to pass to the model checkpoint callback function. The model checkpoint function will always monitor the validation dataset's value of the first metric in the list of accuracy metrics. Set *save_best_only* to 'false' if you would like to save the model after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"model_checkpts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random_seed\n",
    "\n",
    "An integer to use for a random seed, in order to enable the reproduction of training runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"random_seed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the RAMP training code\n",
    "\n",
    "Now that we know what's in the configuration file, we'll walk through the steps of setting up the training and running it.  \n",
    "\n",
    "### Step 1: Check your GPU setup and access.\n",
    "\n",
    "Before you begin training, you'll want to check whether Tensorflow has access to your GPU to run training. The code below helps you diagnose problems.\n",
    "\n",
    "Below, we directly check Tensorflow access to the GPUs.\n",
    "You should see as many PhysicalDevice listings as you have GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up, or disable, logging to Tensorboard and the experiment log. \n",
    "\n",
    "Set the timestamp for the current experiment, and add it to the training run configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_experiment = False\n",
    "if \"discard_experiment\" in cfg:\n",
    "    discard_experiment = cfg[\"discard_experiment\"]\n",
    "    \n",
    "cfg[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "cfg[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Steps 2-5: specifying and compiling the building detection model.\n",
    "\n",
    "### Step 2: Construct the loss function for the training run.\n",
    "\n",
    "The user must specify a single loss function for the training run.\n",
    "\n",
    "To do this, the user specifies a function from the *loss_constructors* module that will be used to construct the loss function. This is necessary because constructing the loss function will frequently require additional parameters to be passed in; for example, a weighted loss function (for example, one that penalizes incorrect boundary pixels more heavily than incorrect background pixels) will need the class weights to be passed in at the time of its construction. These weights must also be defined in the configuration file.\n",
    "\n",
    "This method of constructing a function dynamically is used repeatedly in the ramp code, as you'll see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify a constructor that will construct the loss function\n",
    "get_loss_fn_name = cfg[\"loss\"][\"get_loss_fn_name\"]\n",
    "get_loss_fn = getattr(loss_constructors, get_loss_fn_name)\n",
    "print(f\"Loss function constructor: {get_loss_fn.__name__}\")\n",
    "\n",
    "# Construct the loss function \n",
    "loss_fn = get_loss_fn(cfg)\n",
    "print(f\"Loss function: {loss_fn.__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: construct the accuracy metrics for the training run.\n",
    "\n",
    "While the neural network model uses only one loss function in training, you can specify more than one accuracy metric to track. You should always specify at least one accuracy measure to track. \n",
    "\n",
    "As with the loss function, the user must specify the constructor functions that will create the desired accuracy functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_metrics = []\n",
    "if cfg[\"metrics\"][\"use_metrics\"]:\n",
    "\n",
    "    get_metrics_fn_names = cfg[\"metrics\"][\"get_metrics_fn_names\"]\n",
    "    get_metrics_fn_parms = cfg[\"metrics\"][\"metrics_fn_parms\"]\n",
    "    \n",
    "    for get_mf_name, mf_parms in zip(get_metrics_fn_names, get_metrics_fn_parms):\n",
    "        get_metric_fn = getattr(metric_constructors, get_mf_name)\n",
    "        print (f\"Metric constructor function: {get_metric_fn.__name__}\")\n",
    "        metric_fn = get_metric_fn(mf_parms)\n",
    "        the_metrics.append(metric_fn)\n",
    "\n",
    "# Print the list of accuracy metrics\n",
    "print(f\"Accuracy metrics: {[fn.name for fn in the_metrics]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: construct the optimizer for the training run\n",
    "\n",
    "Model training proceeds by making incremental adjustments to the model parameters that attempt to minimize (or optimize) the value of the user's chosen loss function. The choice of size and direction for those incremental adjustments is made by an optimization algorithm. \n",
    " \n",
    "The user must specify the construction function that will construct the optimizer algorithm. In this example, the configuration file also contains a parameter, the 'learning_rate', which is used in the construction of the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### construct optimizer ####\n",
    "\n",
    "get_optimizer_fn_name = cfg[\"optimizer\"][\"get_optimizer_fn_name\"]\n",
    "get_optimizer_fn = getattr(optimizer_constructors, get_optimizer_fn_name)\n",
    "print (f\"Optimizer constructor: {get_optimizer_fn.__name__}\")\n",
    "\n",
    "\n",
    "optimizer = get_optimizer_fn(cfg)\n",
    "print(optimizer)\n",
    "print(float(optimizer.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Construct the model.\n",
    "\n",
    "The code below optionally constructs a new model, or uses a saved model from a previous training run. \n",
    "\n",
    "##### constructing from a saved model \n",
    "\n",
    "If a saved model is used, the model is loaded from a location in the configuration file. Note that all directories in the configuration file are given relative to the RAMP_HOME environment variable, which must be defined in every environment that runs ramp code. \n",
    "\n",
    "If you set *save_optimizer_state* to True in the configuration file, ramp training will proceed using the configuration that was used to train the saved model. This will cause the above specification of loss function, accuracy metrics, and optimizer to be bypassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = None\n",
    "\n",
    "print(cfg[\"saved_model\"][\"use_saved_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if cfg[\"saved_model\"][\"use_saved_model\"]:\n",
    "\n",
    "    # load (construct) the model\n",
    "    model_path = Path(RAMP_HOME) / cfg[\"saved_model\"][\"saved_model_path\"]\n",
    "    print(f\"Model: importing saved model {str(model_path)}\")\n",
    "    the_model = tf.keras.models.load_model(model_path)\n",
    "    assert the_model is not None, f\"the saved model was not constructed: {model_path}\"\n",
    "\n",
    "    if not cfg[\"saved_model\"][\"save_optimizer_state\"]:\n",
    "        # If you don't want to save the original state of training, recompile the model.\n",
    "        the_model.compile(optimizer = optimizer, \n",
    "            loss=loss_fn,\n",
    "            metrics = the_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### constructing a new model\n",
    "\n",
    "If a new model is created, the user specifies the constructor for the model. The code below constructs the model, and compiles it (i.e., sets an optimizer, loss function, and metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg[\"saved_model\"][\"use_saved_model\"]:\n",
    "    get_model_fn_name = cfg[\"model\"][\"get_model_fn_name\"]\n",
    "    get_model_fn = getattr(model_constructors, get_model_fn_name)\n",
    "    print(f\"Model constructor: {get_model_fn.__name__}\")\n",
    "    the_model = get_model_fn(cfg)\n",
    "\n",
    "    assert the_model is not None, f\"the model was not constructed: {model_path}\"\n",
    "    the_model.compile(optimizer = optimizer, \n",
    "        loss=loss_fn,\n",
    "        metrics = the_metrics)\n",
    "\n",
    "print(the_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 6-10: Prepare the training data.\n",
    "\n",
    "#### Step 6: Specify directories to use for training and validation data. \n",
    "\n",
    "The training process uses a training data set, from which it learns the model, and a validation data set, which is used to track how well the trained model performs on data it hasn't been trained with. Both are critical. \n",
    "\n",
    "Each data set contains sample image chips and matching truth datasets, stored in geojson files. The base filenames of each matching image and polygon file must match uniquely, e.g., '130bfe-210.tif' and '130bfe-210.geojson'. \n",
    "\n",
    "The paths to these datasets are defined in the configuration file, relative to the RAMP_HOME environment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### define data directories ####\n",
    "train_img_dir = Path(RAMP_HOME) / cfg[\"datasets\"][\"train_img_dir\"]\n",
    "train_mask_dir = Path(RAMP_HOME) / cfg[\"datasets\"][\"train_mask_dir\"]\n",
    "val_img_dir = Path(RAMP_HOME) / cfg[\"datasets\"][\"val_img_dir\"]\n",
    "val_mask_dir = Path(RAMP_HOME) / cfg[\"datasets\"][\"val_mask_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: set up the augmentations (i.e., image transformations) that will be applied to the training data.\n",
    "\n",
    "The *get_augmentation_fn()* constructs a sequence of image transformation functions that will be applied to the training data only (not the validation data). This increases the variability in the data that the model sees during training.\n",
    "\n",
    "Note that each image transformation function in the list below requires its internal parameters to be set during construction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get the augmentation transform ####\n",
    "aug = None\n",
    "if cfg[\"augmentation\"][\"use_aug\"]:\n",
    "    aug = get_augmentation_fn(cfg)\n",
    "    print(aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Set runtime parameters, and add them to the configuration data.\n",
    "\n",
    "The number of training iterations per epoch depends on the total quantity of training data chips, and on the batch size. \n",
    "\n",
    "The data generator (which 'feeds' data to the model during training) must know the sizes of its input and output images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = cfg[\"batch_size\"]\n",
    "input_img_shape = cfg[\"input_img_shape\"]\n",
    "output_img_shape = cfg[\"output_img_shape\"]\n",
    "\n",
    "n_training = get_num_files(train_img_dir, \"*.tif\")\n",
    "n_val = get_num_files(val_img_dir, \"*.tif\")\n",
    "steps_per_epoch = n_training // batch_size\n",
    "validation_steps = n_val // batch_size\n",
    "\n",
    "# add these back to the config \n",
    "# in case they are needed by callbacks\n",
    "cfg[\"runtime\"] = {}\n",
    "cfg[\"runtime\"][\"n_training\"] = n_training\n",
    "cfg[\"runtime\"][\"n_val\"] = n_val\n",
    "cfg[\"runtime\"][\"steps_per_epoch\"] = steps_per_epoch\n",
    "cfg[\"runtime\"][\"validation_steps\"] = validation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: set up training and validation data 'feeds'.\n",
    "\n",
    "Notice the call to construct training batches is different when augmentation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = None\n",
    "\n",
    "if aug is not None:\n",
    "    train_batches = training_batches_from_gtiff_dirs(train_img_dir, \n",
    "                                                train_mask_dir, \n",
    "                                                batch_size, \n",
    "                                                input_img_shape, \n",
    "                                                output_img_shape, \n",
    "                                                transforms = aug)\n",
    "else:\n",
    "    train_batches = training_batches_from_gtiff_dirs(train_img_dir, \n",
    "                                                train_mask_dir, \n",
    "                                                batch_size, \n",
    "                                                input_img_shape, \n",
    "                                                output_img_shape)\n",
    "\n",
    "assert train_batches is not None, \"training batches were not constructed\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = test_batches_from_gtiff_dirs(val_img_dir, \n",
    "                                                val_mask_dir, \n",
    "                                                batch_size, \n",
    "                                                input_img_shape, \n",
    "                                                output_img_shape)\n",
    "\n",
    "assert val_batches is not None, \"validation batches were not constructed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 11-13: Construct callbacks.\n",
    "\n",
    "A list of callback functions will be passed to the training process. \n",
    "\n",
    "### Step 11: Define all experiment logging and model checkpoint callbacks.\n",
    "\n",
    "These callbacks are essential and used in every training run (unless it is a throwaway).\n",
    "\n",
    "#todo: add instructions for starting tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = []\n",
    "\n",
    "if not discard_experiment:\n",
    "\n",
    "    # get model checkpoint callback\n",
    "    if cfg[\"model_checkpts\"][\"use_model_checkpts\"]:\n",
    "        get_model_checkpt_callback_fn_name = cfg[\"model_checkpts\"][\"get_model_checkpt_callback_fn_name\"]\n",
    "        get_model_checkpt_callback_fn = getattr(callback_constructors, get_model_checkpt_callback_fn_name)\n",
    "        callbacks_list.append(get_model_checkpt_callback_fn(cfg))\n",
    "        print(f\"model checkpoint callback constructor:{get_model_checkpt_callback_fn.__name__}\")\n",
    "\n",
    "    # get tensorboard callback\n",
    "    if cfg[\"tensorboard\"][\"use_tb\"]:\n",
    "        get_tb_callback_fn_name = cfg[\"tensorboard\"][\"get_tb_callback_fn_name\"]\n",
    "        get_tb_callback_fn = getattr(callback_constructors, get_tb_callback_fn_name)\n",
    "        callbacks_list.append(get_tb_callback_fn(cfg))\n",
    "        print(f\"tensorboard callback constructor: {get_tb_callback_fn.__name__}\")\n",
    "\n",
    "    # get tensorboard model prediction logging callback \n",
    "    if cfg[\"prediction_logging\"][\"use_prediction_logging\"]:\n",
    "        assert cfg[\"tensorboard\"][\"use_tb\"], 'Tensorboard logging must be turned on to enable prediction logging'\n",
    "        get_prediction_logging_fn_name = cfg[\"prediction_logging\"][\"get_prediction_logging_fn_name\"]\n",
    "        get_prediction_logging_fn = getattr(callback_constructors, get_prediction_logging_fn_name)\n",
    "        callbacks_list.append(get_prediction_logging_fn(the_model, cfg))\n",
    "        print(f\"prediction logging callback constructor: {get_prediction_logging_fn.__name__}\")\n",
    "\n",
    "# free up RAM\n",
    "keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Define the early stopping callback if you're using it.\n",
    "\n",
    "You often will be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"early_stopping\"][\"use_early_stopping\"]:\n",
    "    print(\"Using early stopping\")\n",
    "    callbacks_list.append(callback_constructors.get_early_stopping_callback_fn(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Define the Cyclic Learning Rate Scheduler callback if you're using it. \n",
    "\n",
    "You probably won't want to do this, but it's included here for completeness. \n",
    "\n",
    "You'll get an error if you try to use a cyclic learning rate scheduler together with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg[\"cyclic_learning_scheduler\"][\"use_clr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cyclic learning scheduler callback\n",
    "if cfg[\"cyclic_learning_scheduler\"][\"use_clr\"]:\n",
    "    assert not cfg[\"early_stopping\"][\"use_early_stopping\"], \"cannot use early_stopping with cycling_learning_scheduler\"\n",
    "    get_clr_callback_fn_name = cfg[\"cyclic_learning_scheduler\"][\"get_clr_callback_fn_name\"]\n",
    "    get_clr_callback_fn = getattr(callback_constructors, get_clr_callback_fn_name)\n",
    "    callbacks_list.append(get_clr_callback_fn(cfg))\n",
    "    print(f\"CLR callback constructor: {get_clr_callback_fn.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Train the model.\n",
    "\n",
    "The 'history' return value is a data structure containing training-time information, such as the values of loss and accuracy metrics after every epoch. Most of what it contains is viewable in the Tensorboard application.\n",
    "\n",
    "The user must specify the number of epochs to run. If early stopping is used, the full number of epochs may not be run. \n",
    "\n",
    "Note: on Google colab, the first run through the training and validation data may be very slow. They speed up and go very quickly once the data has been read and cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = cfg[\"num_epochs\"]\n",
    "print(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = the_model.fit(train_batches, \n",
    "        epochs=n_epochs, \n",
    "        steps_per_epoch=steps_per_epoch, \n",
    "        validation_data=val_batches, \n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: log experiment information to a central location.\n",
    "\n",
    "Ramp has a simple mechanism for storing experiment information to a central CSV file, the path to which is specified (like everything else) in the configuration file. Fields to be logged are specified in the 'logging' configuration block, under 'fields_to_log'. \n",
    "\n",
    "Fields to be logged should be uniquely named (or you might not get the records you wanted!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not discard_experiment and cfg[\"logging\"][\"log_experiment\"]:\n",
    "    exp_log_path = str(Path(RAMP_HOME)/cfg[\"logging\"][\"experiment_log_path\"])\n",
    "    print(f\"logging experiment to: {exp_log_path}\")\n",
    "\n",
    "    # log fields chosen in the configuration file\n",
    "    # the fields must be uniquely named\n",
    "    fields_to_log = cfg[\"logging\"][\"fields_to_log\"]\n",
    "    exp_log = dict()\n",
    "    for field_key in fields_to_log:\n",
    "        field_val = lf.locate_field(cfg, field_key)\n",
    "        if not isinstance(field_val, dict):\n",
    "            exp_log[field_key] = field_val\n",
    "    the_argmax, best_val_acc = get_best_model_value_and_epoch(history)\n",
    "    exp_log[lf.BEST_MODEL_VALUE] = best_val_acc\n",
    "    exp_log[lf.BEST_MODEL_EPOCH] = the_argmax\n",
    "    log_experiment_to_file(exp_log, exp_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
