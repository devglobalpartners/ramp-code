{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3389f4",
   "metadata": {},
   "source": [
    "## Demonstration: augmentation of input chips and labels\n",
    "\n",
    "This notebook demonstrates augmentation of input chips and labels. This is a device that is used in training of deep learning models to 'stretch' limited data so that the model sees training samples with slight differences, rather than the same samples over and over. \n",
    "\n",
    "The notebook demonstrates setting up the ramp data generator for training, then applying image transformations randomly to the images, and displaying the results. Thus, you can see what augmentation is actually doing to the images. \n",
    "\n",
    "Augmentations were used successfully in the [Google building footprints over Africa project](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiG36Xji5z4AhXlAJ0JHdi_BKwQFnoECAUQAQ&url=https%3A%2F%2Fsites.research.google%2Fopen-buildings%2F&usg=AOvVaw0f_y7vp7CYXhnQFgB82uJR) to improve building model extraction quality. The specific image transformations used in that project were:\n",
    "\n",
    "- random cropping of images\n",
    "- horizontal and vertical image flipping\n",
    "- random rotations\n",
    "- random modifications to image hue, brightness and contrast. \n",
    "\n",
    "They found that random modifications to color helped the model deal better with variable image quality and atmospheric conditions. \n",
    "\n",
    "The chip and mask images at the bottom of this notebook show the effects of image augmentations of the above types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d286571",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3461bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # only print errors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a712be",
   "metadata": {},
   "source": [
    "### Import ramp code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9aa328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "RAMP_HOME = os.environ[\"RAMP_HOME\"]\n",
    "from ramp.data_mgmt.data_generator import training_batches_from_gtiff_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base = os.path.join(RAMP_HOME, 'ramp-code/notebooks/sample-data/training_data')\n",
    "image_dir = os.path.join(train_base, \"chips\")\n",
    "mask_dir = os.path.join(train_base, \"multimasks\")\n",
    "batch_size = 12\n",
    "input_image_size = (256,256)\n",
    "output_image_size = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf99613",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = training_batches_from_gtiff_dirs(image_dir, \n",
    "                                                 mask_dir, \n",
    "                                                 batch_size, \n",
    "                                                 input_image_size, \n",
    "                                                 output_image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chips = None\n",
    "masks = None\n",
    "for items in train_batches.take(1):\n",
    "    chips = items[0]\n",
    "    masks = items[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, mask, original_image=None, original_mask=None):\n",
    "    fontsize = 18\n",
    "    \n",
    "    if original_image is None and original_mask is None:\n",
    "        f, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(mask)\n",
    "    else:\n",
    "        f, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "        ax[0, 0].imshow(original_image)\n",
    "        ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
    "        \n",
    "        ax[1, 0].imshow(original_mask)\n",
    "        ax[1, 0].set_title('Original mask', fontsize=fontsize)\n",
    "        \n",
    "        ax[0, 1].imshow(image)\n",
    "        ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n",
    "        \n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5554c5a",
   "metadata": {},
   "source": [
    "### Define image augmentations  \n",
    "\n",
    "When random rotation is applied to images, some information at the edges of the image is rotated out of view, and new areas without data are rotated into view. It is important that this new data is identified as nodata (0-0-0) in the image chips, and as non-buildings in the mask chips (0). This is what the 'value' and 'mask_value' arguments to the Rotate transform specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from cv2 import BORDER_CONSTANT, INTER_NEAREST\n",
    "\n",
    "aug = A.Compose([\n",
    "                A.Rotate(\n",
    "                    border_mode=BORDER_CONSTANT, \n",
    "                    interpolation=INTER_NEAREST, \n",
    "                    value=(0.0,0.0,0.0), \n",
    "                    mask_value = 0, \n",
    "                    p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                    contrast_limit=0.2, \n",
    "                    brightness_by_max=True, \n",
    "                    p=0.9)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = np.zeros(chips.shape)\n",
    "new_mask = np.zeros(masks.shape)\n",
    "for ii in range(batch_size):\n",
    "    the_img = chips[ii,:,:,:]\n",
    "    the_mask = masks[ii,:,:,0]\n",
    "    augmented = aug(image=the_img.numpy(), mask=the_mask.numpy())\n",
    "    new_img[ii,:,:,:] = augmented['image']\n",
    "    new_mask[ii,:,:,0] = augmented['mask']\n",
    "    visualize(new_img[ii,:,:,:], new_mask[ii,:,:,0], the_img.numpy(), the_mask.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61e181",
   "metadata": {},
   "source": [
    "##### Created for ramp project, August 2022\n",
    "##### Author: carolyn.johnston@dev.global"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
