{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMP PROJECT: Independent labeler comparison test\n",
    "\n",
    "In an effort to understand labeler variation before undertaking the huge data processing task required for the ramp project, we had two labelers independently collect all the buildings over an AOI, and checked to see how much the datasets differ.\n",
    "\n",
    "In this notebook, we measure the interannotator agreement between two label sets collected over the same area. \n",
    "\n",
    "**To make a long story short -- F1 using IoU@0.5 for this data set is only 0.66, or 66%!** \n",
    "\n",
    "In short, independent labelers only agree on about 2/3rds of the buildings they label in a previously unfamiliar area! This gives an idea of what the 'human accuracy' of a building labeler on a dataset, over an area where the labelers are unfamiliar with the building and habitation patterns.\n",
    "\n",
    "There are several lessons learned from this experiment:\n",
    "\n",
    "1. With machine learning, we are trying to meet or exceed human performance on a task, and this result is a rough measure of human-level accuracy without guidance. This demonstration shows that without some guidance, human-level accuracy is not such a high bar to cross!\n",
    "\n",
    "2. Some loss of interannotator agreement can be expected when labelers are unfamiliar with the country's building and habitation practice. For example, in many areas studied by the ramp team, the buildings were not completed; there was no roof covering the top floors. In the United States, such a building would not be inhabited. One of our (well-traveled) team members, however, knew that such buildings in this location routinely have inhabitants on the lower stories, so these buildings needed to be captured by the labelers.\n",
    "\n",
    "3. Even in cases where the building and habitation practices in-country are known to the labelers, differences in individual labelers' methods can have a harmful impact on the trained model. For example, one labeler captured building facades in the above image, and the other did not; this difference is bound to weaken the model's performance. Therefore, it is critical to provide detailed labeling guidelines, and require the labelers to follow them. \n",
    "\n",
    "It would be very interesting to compare this interannotator agreement score with that of a team of labelers who are familiar with the building practices in an area of interest. If, as we suspect, it is much higher, that is a strong argument for employing local data labelers wherever possible.\n",
    "\n",
    "#### the details\n",
    "\n",
    "The accuracy metric we use is the same as the metric used in Spacenet 2: we consider a building to have been detected by the algorithm if the region of overlap between the truth and test buildings is greater than .5 of the union of the two building sets. This measure is called Intersection-over-Union at 0.5, or IoU@0.5.  \n",
    "\n",
    "See [this article](https://medium.com/the-downlinq/the-spacenet-metric-612183cc2ddb) for a description of IoU@0.5, and how it is used to calculate the standard measures of accuracy: precision, recall, and F1.\n",
    "\n",
    "Two different labelers collected buildings over the same area in Dhaka, Bangladesh, at the beginning of the RAMP project. The two labelers were members of the ramp team in the United States. The natural differences between labelers are evident in the screenshot below. \n",
    "\n",
    "![interannotator test datasets](images/interannotator_qgis.png)\n",
    "\n",
    "Interannotator agreement metrics (i.e., precision, recall, and F1 metrics based on IoU@0.5) were calculated using the code in this notebook. The same metrics can also be calculated using the python script *calculate_accuracy_iou.py*, located in the scripts directory of the ramp codebase.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from ramp.utils.eval_utils import get_iou_accuracy_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = Path('sample-data/interannotator_check')\n",
    "print(basedir.is_dir())\n",
    "seamus_labels = basedir / \"seamus-labels.geojson\"\n",
    "matt_labels = basedir / \"matt-labels.geojson\"\n",
    "gpd_seamus = gpd.read_file(str(seamus_labels))\n",
    "gpd_matt = gpd.read_file(str(matt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataset 1\n",
    "gpd_seamus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_truth = gpd_matt\n",
    "gpd_test = gpd_seamus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = get_iou_accuracy_metrics(gpd_test, gpd_truth, 0.5)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Created for ramp project, August 2022\n",
    "##### Author: carolyn.johnston@dev.global"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
